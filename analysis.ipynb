{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxsQRHUs6WWh"
      },
      "outputs": [],
      "source": [
        "# Group members: Lilian Yu (jiajieyu), Ziyi Shen (ziyis), Tristan Zhang (jinyang3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_DxiNk01cQx"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pprint\n",
        "# import project_helper\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndkGzztx1g9i"
      },
      "outputs": [],
      "source": [
        "# project_helper\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "\n",
        "\n",
        "class SecAPI(object):\n",
        "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
        "\n",
        "    @staticmethod\n",
        "    @sleep_and_retry\n",
        "    # Dividing the call limit by half to avoid coming close to the limit\n",
        "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
        "    def _call_sec(url):\n",
        "        return requests.get(url)\n",
        "\n",
        "    def get(self, url):\n",
        "        return self._call_sec(url).text\n",
        "\n",
        "\n",
        "def print_ten_k_data(ten_k_data, fields, field_length_limit=50):\n",
        "    indentation = '  '\n",
        "\n",
        "    print('[')\n",
        "    for ten_k in ten_k_data:\n",
        "        print_statement = '{}{{'.format(indentation)\n",
        "        for field in fields:\n",
        "            value = str(ten_k[field])\n",
        "\n",
        "            # Show return lines in output\n",
        "            if isinstance(value, str):\n",
        "                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n",
        "            else:\n",
        "                value_str = str(value)\n",
        "\n",
        "            # Cut off the string if it gets too long\n",
        "            if len(value_str) > field_length_limit:\n",
        "                value_str = value_str[:field_length_limit] + '...'\n",
        "\n",
        "            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n",
        "\n",
        "        print_statement += '},'\n",
        "        print(print_statement)\n",
        "    print(']')\n",
        "\n",
        "\n",
        "def plot_similarities(similarities_list, dates, title, labels):\n",
        "    assert len(similarities_list) == len(labels)\n",
        "\n",
        "    plt.figure(1, figsize=(10, 7))\n",
        "    for similarities, label in zip(similarities_list, labels):\n",
        "        plt.title(title)\n",
        "        plt.plot(dates, similarities, label=label)\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=90)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSEUh6e1l5Y",
        "outputId": "e9d1d311-4469-42b8-b1f8-efb4d73855b2"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAPDHFLs59UV"
      },
      "outputs": [],
      "source": [
        "#cik_lookup = {\n",
        "  #  'AMZN': '0001018724',\n",
        " #   'BMY': '0000014272',   \n",
        "  #  'CNP': '0001130310',\n",
        "  #  'CVX': '0000093410',\n",
        " #   'FL': '0000850209',\n",
        " #   'FRT': '0000034903',\n",
        " #   'HON': '0000773840'}\n",
        "#cik_lookup = {\n",
        "  # 'AMZN': '0001018724',\n",
        " #   'BMY': '0000014272'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "AUXdLkoLQRSt",
        "outputId": "40da43fe-e416-4e16-d85a-f5e9608a6655"
      },
      "outputs": [],
      "source": [
        "#import tick and cik\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#import io\n",
        "df2 = pd.read_csv('sp500_w_addl_id_with_cik.csv')\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PGjEJ2cRQSIk",
        "outputId": "22fa27be-b5ad-41c6-c16b-2b1503b44c3a"
      },
      "outputs": [],
      "source": [
        "df = df2[['date','ticker', 'cik']]\n",
        "df=df.loc[df['date']>='2017-01-01']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXeacewqQSq0",
        "outputId": "8efb9679-430c-4152-ded7-48a0b2d98b98"
      },
      "outputs": [],
      "source": [
        "tickers = df.groupby('date')['ticker'].apply(list)\n",
        "ciks = df.groupby('date')['cik'].apply(list)\n",
        "tickers,ciks\n",
        "#sp500=df.groupby('ticker')['date'].apply(list)\n",
        "#sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOmICRMARmXI"
      },
      "outputs": [],
      "source": [
        "#tickers.keys()\n",
        "#\n",
        "#df.loc[df['cik'].isna()] AA and DPS have no cik listed\n",
        "df = df[df.cik.notnull()]\n",
        "#df['cik'].astype(int)\n",
        "#cik_lookup=dict(zip(df.ticker[:5], df['cik'][:5].astype(int)))\n",
        "#cik_lookup                                                          ##pick first 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC7pLZK6ZJRU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "look= df.groupby('ticker')['cik'].apply(list)\n",
        "look=look.to_dict()\n",
        "#lookup=dict(zip(look.ticker, look['ticker'][0].astype(int)))\n",
        "#lookup\n",
        "cik_lookupss={}\n",
        "for ticker, ciks in look.items():\n",
        "    cik_lookupss[ticker]=int(ciks[0])\n",
        "##cik_lookupss.keys()     ##656 companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No4NUs51Qfqa"
      },
      "outputs": [],
      "source": [
        "dates = df2['date'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9LcbbacZJRV"
      },
      "outputs": [],
      "source": [
        "ALL_ticker=list(cik_lookupss.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF08C0TMZJRW"
      },
      "outputs": [],
      "source": [
        "########\n",
        "num = 450\n",
        "cik_lookup= { key: cik_lookupss[key] for key in ALL_ticker[num:(num+50)] }   #first 50 comps\n",
        "#cik_lookup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHRT2fwL1oi_",
        "outputId": "7f62e0a9-5ca2-425e-cf8d-ff9408e133ac"
      },
      "outputs": [],
      "source": [
        "######10K\n",
        "\n",
        "sec_api = SecAPI()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "HEADER = {'Host': 'www.sec.gov', 'Connection': 'close',\n",
        "         'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest',\n",
        "         'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
        "         }\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "  rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "      '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "      .format(cik, doc_type, start, count)\n",
        "  sec_data = requests.get(rss_url,headers=HEADER)\n",
        "  feed = BeautifulSoup(sec_data.content, 'html')\n",
        "  entries = [\n",
        "      (\n",
        "          entry.content.find('filing-href').getText(),\n",
        "          entry.content.find('filing-type').getText(),\n",
        "          entry.content.find('filing-date').getText())\n",
        "      for entry in feed.find_all('entry')]\n",
        "  return entries\n",
        "\n",
        "example_ticker = ALL_ticker[num]\n",
        "sec_data = {}   #dict 10K\n",
        "for ticker, cik in cik_lookup.items():\n",
        "    sec_data[ticker] = get_sec_data(cik, '10-K')[0:5] ##\n",
        "pprint.pprint(sec_data[example_ticker])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGLHLhzzZd",
        "outputId": "f5d4a103-604d-4bb4-e83f-8376ee697a31"
      },
      "outputs": [],
      "source": [
        "#####10Q\n",
        "\n",
        "sec_api = SecAPI()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "HEADER = {'Host': 'www.sec.gov', 'Connection': 'close',\n",
        "         'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest',\n",
        "         'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
        "         }\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "  rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "      '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "      .format(cik, doc_type, start, count)\n",
        "  sec_data = requests.get(rss_url,headers=HEADER)\n",
        "  feed = BeautifulSoup(sec_data.content, 'html')\n",
        "  entries = [\n",
        "      (\n",
        "          entry.content.find('filing-href').getText(),\n",
        "          entry.content.find('filing-type').getText(),\n",
        "          entry.content.find('filing-date').getText())\n",
        "      for entry in feed.find_all('entry')]\n",
        "  return entries\n",
        "\n",
        "example_ticker = ALL_ticker[num]\n",
        "sec_data_10Q = {}     #dict 10Q\n",
        "for ticker, cik in cik_lookup.items():\n",
        "    sec_data_10Q[ticker] = get_sec_data(cik, '10-Q')[2:18]\n",
        "pprint.pprint(sec_data_10Q[example_ticker])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXiM7iAlOxVP"
      },
      "outputs": [],
      "source": [
        "#periods=pd.DataFrame()\n",
        "#periods['End']=tickers.keys()\n",
        "#periods['Start']=periods.End.shift(1)\n",
        "#periods['Start'][0]='2017-01-01'\n",
        "#periods.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUR-uOTZ14jM"
      },
      "outputs": [],
      "source": [
        "#for ticker in cik_lookup.keys():\n",
        "#  print(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LATVIFE72AJC"
      },
      "outputs": [],
      "source": [
        "for ticker in cik_lookup.keys():     ##10K  keep all in the time horizon 5 years\n",
        "  for item in sec_data[ticker]:\n",
        "    if (item[-1]<'2017-01-03') or (item[-1]>'2021-12-31'):\n",
        "      sec_data[ticker].remove(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAoxO2LQ9BkZ"
      },
      "outputs": [],
      "source": [
        "for ticker in cik_lookup.keys():   \n",
        "  for item in sec_data_10Q[ticker]:     ##10Q  keep all in the time horizon\n",
        "    if (item[-1]<'2017-01-03') or (item[-1]>'2021-12-31'):\n",
        "      sec_data_10Q[ticker].remove(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McujlMux9PzC"
      },
      "outputs": [],
      "source": [
        "#sec_data_10Q[ticker]  Now all 10Q files are in date range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YXgvaab6VmJ",
        "outputId": "b2fdc138-f959-4437-bdf2-6c02c7539d8d"
      },
      "outputs": [],
      "source": [
        "####Take long time, could play around for a while\n",
        "\n",
        "\n",
        "raw_fillings_by_ticker = {}     \n",
        "for ticker, data in sec_data.items():   ###get 10K from dict\n",
        "    try:\n",
        "        raw_fillings_by_ticker[ticker] = {}   ###create empty 10K filing\n",
        "        for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
        "            if (file_type == '10-K'):\n",
        "                file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
        "            \n",
        "                raw_fillings_by_ticker[ticker][file_date] = requests.get(file_url,headers=HEADER).text\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvnfPZS3Ikvd",
        "outputId": "22a7761b-f025-440e-9328-83473a6df59b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def get_documents(text):\n",
        "    extracted_docs = []\n",
        "    \n",
        "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
        "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
        "    \n",
        "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
        "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
        "    \n",
        "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
        "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
        "    \n",
        "    return extracted_docs\n",
        "\n",
        "filling_documents_by_ticker = {}\n",
        "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
        "    try:\n",
        "        filling_documents_by_ticker[ticker] = {}\n",
        "        for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
        "            filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('\\n\\n'.join([\n",
        "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
        "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
        "    for doc_i, doc in enumerate(docs)][:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqSFn7HhIpIM"
      },
      "outputs": [],
      "source": [
        "def get_document_type(doc):\n",
        "    \n",
        "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
        "    \n",
        "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
        "    \n",
        "    return doc_type.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLqwqGQ-Iqc1",
        "outputId": "face3a81-48d9-47ca-9af0-aa0cadaac7ec"
      },
      "outputs": [],
      "source": [
        "ten_ks_by_ticker = {}\n",
        "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
        "    try:\n",
        "        ten_ks_by_ticker[ticker] = []\n",
        "        for file_date, documents in filling_documents.items():\n",
        "            for document in documents:\n",
        "                if get_document_type(document) == '10-k':\n",
        "                    ten_ks_by_ticker[ticker].append({\n",
        "                        'cik': cik_lookup[ticker],\n",
        "                        'file': document,\n",
        "                        'file_date': file_date})\n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0teHM3lIzfi"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    \n",
        "    return text\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = remove_html_tags(text)\n",
        "    #text = re.sub(\"[^a-zA-Z]\",\" \",text).split(' ')  #this will take off the release date, do it later before \n",
        "    \n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87UJRzZHKw7X",
        "outputId": "a36af774-e0a1-4fc5-8d22-81ec7213adb7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "####Take long time, could play around for a while  ~10min for 50 comps 10K\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "            ten_k['file_clean'] = clean_text(ten_k['file'])\n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euIbtT5wZJRa",
        "outputId": "75df05d2-6bcc-4e9a-8a2e-5ad10cb538e1",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_date'])\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][0:1], ['file_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nQJxGs7TZJRa",
        "outputId": "7c161bb3-94ac-4013-ce19-4c2c9bb3e5c8"
      },
      "outputs": [],
      "source": [
        "ten_ks_by_ticker[example_ticker][0]['file_date']  ##the 0th company's file date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYQ05QU2K3VV"
      },
      "source": [
        "lemmatize all the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFkYRlDhK1JO",
        "outputId": "150ea1ed-bcc3-4edb-f901-1e70a5a9f3b4"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "def lemmatize_words(words):\n",
        "\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
        "    \n",
        "    return lemmatized_words\n",
        "word_pattern = re.compile('\\w+')\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "            ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcugnds4K_KV"
      },
      "source": [
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdt30h3GK_Av",
        "outputId": "063dce89-1a6c-49db-f98e-cc72187717a0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "            ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('Stop Words Removed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDFgSEhHZJRd",
        "outputId": "b1dc8d29-c77b-4053-e176-85d4a1e55e67"
      },
      "outputs": [],
      "source": [
        "ten_k['file_lemma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf-F87jaZJRd",
        "outputId": "5b26611d-3e2a-4779-8f85-9f84bbff14be",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "###take out strings containing numbers\n",
        "\n",
        "def has_numbers(inputString):\n",
        "    return any(char.isdigit() for char in inputString)\n",
        "\n",
        "def clean_numb(text):\n",
        "    \n",
        "    text = [item for subitem in text for item in subitem.split() if item.isdigit()==False]  \n",
        "    text = [item for subitem in text for item in subitem.split() if has_numbers(item)==False]\n",
        "    return text\n",
        "\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "            ten_k['file_clean_numb'] = clean_numb(ten_k['file_lemma'])\n",
        "    except:\n",
        "        print\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean_numb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qgezqB2ZJRe",
        "outputId": "60f923da-02bc-448b-f0ce-3d34d1274aca"
      },
      "outputs": [],
      "source": [
        "ten_ks_by_ticker[example_ticker][0]['file_clean_numb']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFzTIr7Oljp"
      },
      "source": [
        "Deal with 10Q the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbLTMZaROjsl",
        "outputId": "991f9723-c50b-4d46-a261-5074e713e8e8"
      },
      "outputs": [],
      "source": [
        "#Take long time  11:58-12:13  ~15min for 50comps\n",
        "raw_fillings_by_ticker_10Q = {}     \n",
        "for ticker, data in sec_data_10Q.items():   ###get 10Q from dict\n",
        "    try:\n",
        "        raw_fillings_by_ticker_10Q[ticker] = {}   ###create empty 10Q filing\n",
        "        for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
        "            if (file_type == '10-Q'):\n",
        "                file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
        "            \n",
        "                raw_fillings_by_ticker_10Q[ticker][file_date] = requests.get(file_url,headers=HEADER).text\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker_10Q[example_ticker].values()))[:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBvdac0uOjLv"
      },
      "outputs": [],
      "source": [
        "filling_documents_by_ticker_10Q = {}\n",
        "for ticker, raw_fillings in raw_fillings_by_ticker_10Q.items():\n",
        "    try:\n",
        "        filling_documents_by_ticker_10Q[ticker] = {}\n",
        "        for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
        "            filling_documents_by_ticker_10Q[ticker][file_date] = get_documents(filling)\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('\\n\\n'.join([\n",
        "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
        "    for file_date, docs in filling_documents_by_ticker_10Q[example_ticker].items()\n",
        "    for doc_i, doc in enumerate(docs)][:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yKPe1wnPAFn"
      },
      "outputs": [],
      "source": [
        "#1 min\n",
        "\n",
        "ten_Qs_by_ticker = {}\n",
        "for ticker, filling_documents in filling_documents_by_ticker_10Q.items():\n",
        "    ten_Qs_by_ticker[ticker] = []\n",
        "    for file_date, documents in filling_documents.items():\n",
        "        for document in documents:\n",
        "            if get_document_type(document) == '10-q':\n",
        "                ten_Qs_by_ticker[ticker].append({\n",
        "                    'cik': cik_lookup[ticker],\n",
        "                    'file': document,\n",
        "                    'file_date': file_date})\n",
        "print_ten_k_data(ten_Qs_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])    ###use print_ten_k_data to print 10Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HfYBzGEGPA9I"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cleaning SBUX 10-Qs:  53%|█████▎    | 8/15 [00:14<00:13,  1.96s/10-Q]"
          ]
        }
      ],
      "source": [
        "\n",
        "#Take long time   1:35pm-1:58  20min or 50comps\n",
        "\n",
        "##slice_tenQ=list(ten_Qs_by_ticker.keys())\n",
        "##ten_Qs_by_ticker_sliced= {key: ten_Qs_by_ticker[key] for key in slice_tenQ[:9] }    #ADBE is indexed-9\n",
        "\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    try:\n",
        "        for ten_Q in tqdm(ten_Qs, desc='Cleaning {} 10-Qs'.format(ticker), unit='10-Q'):\n",
        "            ten_Q['file_clean'] = clean_text(ten_Q['file'])\n",
        "    except:\n",
        "        print(ticker)\n",
        "        #del ten_Qs_by_ticker[ticker]\n",
        "        \n",
        "print_ten_k_data(ten_Qs_by_ticker[example_ticker][:5], ['file_clean'])\n",
        "\n",
        "#enountered error at ADBE, so skipped ADBE's 10Q \n",
        "#next time encounter error just skip and print tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDBJmHYMZJRf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ten_Qs_by_ticker.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJfehQV7PGRe"
      },
      "outputs": [],
      "source": [
        "word_pattern = re.compile('\\w+')\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    try:\n",
        "        for ten_Q in tqdm(ten_Qs, desc='Lemmatize {} 10-Qs'.format(ticker), unit='10-Q'):\n",
        "            ten_Q['file_lemma'] = lemmatize_words(word_pattern.findall(ten_Q['file_clean']))\n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data(ten_Qs_by_ticker[example_ticker][:5], ['file_lemma'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTE4JWM9PGw-",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    try:\n",
        "        for ten_Q in tqdm(ten_Qs, desc='Remove Stop Words for {} 10-Qs'.format(ticker), unit='10-Q'):\n",
        "            ten_Q['file_lemma'] = [word for word in ten_Q['file_lemma'] if word not in lemma_english_stopwords]\n",
        "    except:\n",
        "        print(ticker)\n",
        "print('Stop Words Removed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6JQuLLjPHGN"
      },
      "outputs": [],
      "source": [
        "def clean_numbQ(text):\n",
        "    \n",
        "    text = [item for subitem in text for item in subitem.split() if item.isdigit()==False]  \n",
        "    text = [item for subitem in text for item in subitem.split() if has_numbers(item)==False]\n",
        "    return text\n",
        "\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    try:\n",
        "        for ten_Q in tqdm(ten_Qs, desc='Cleaning {} 10-Qs'.format(ticker), unit='10-Q'):\n",
        "            ten_Q['file_clean_numb'] = clean_numbQ(ten_Q['file_lemma'])\n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data(ten_Qs_by_ticker[example_ticker][:5], ['file_clean_numb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiments = ['negative']\n",
        "\n",
        "Fin_Neg=pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')\n",
        "#Fin_Neg=Fin_Neg.loc[Fin_Neg['Negative']>=2009]\n",
        "\n",
        "\n",
        "sentiment_df = Fin_Neg\n",
        "sentiment_df.columns = [column.lower() for column in sentiment_df.columns] # Lowercase the columns for ease of use\n",
        "\n",
        "# Remove unused information\n",
        "sentiment_df = sentiment_df[sentiments + ['word']]\n",
        "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
        "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
        "\n",
        "# Apply the same preprocessing to these words as the 10-k words\n",
        "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
        "sentiment_df = sentiment_df.drop_duplicates('word')\n",
        "\n",
        "\n",
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl5weRF_ZJRj"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#import project_helper\n",
        "def get_bag_of_words(sentiment_words, docs):\n",
        "\n",
        "    vec = CountVectorizer(vocabulary=sentiment_words)\n",
        "    vectors = vec.fit_transform(docs)\n",
        "    words_list = vec.get_feature_names()\n",
        "    bag_of_words = np.zeros([len(docs), len(words_list)])\n",
        "    \n",
        "    for i in range(len(docs)):\n",
        "        bag_of_words[i] = vectors[i].toarray()[0]\n",
        "    return bag_of_words.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK9F_Pl6ZJRk"
      },
      "outputs": [],
      "source": [
        "sentiment_bow_ten_ks = {}   #####setup ONCE?\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_k['file_clean_numb']) for ten_k in ten_ks]\n",
        "\n",
        "    sentiment_bow_ten_ks[ticker] = {sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
        "       for sentiment in sentiments}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeMWCvNlZJRk"
      },
      "outputs": [],
      "source": [
        "def get_negative_word_countK(ticker):\n",
        "    return np.cumsum(sentiment_bow_ten_ks[ticker]['negative'], axis=1)[:,-1]\n",
        "\n",
        "\n",
        "def get_total_word_countK(ticker):\n",
        "    total=[0]*len(ten_ks_by_ticker[ticker])\n",
        "    for i in range(len(ten_ks_by_ticker[ticker])):\n",
        "        total[i]=len(ten_ks_by_ticker[ticker][i]['file_clean_numb'])\n",
        "    return total   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-fUDZ8t9vKw"
      },
      "outputs": [],
      "source": [
        "FNproporK={}\n",
        "for ticker in ten_ks_by_ticker.keys():\n",
        "    FNproporK[ticker]=get_negative_word_countK(ticker)/get_total_word_countK(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8VRqWiIZJRl"
      },
      "outputs": [],
      "source": [
        "def get_file_dateK(ticker):\n",
        "    fdate=[0]*len(ten_ks_by_ticker[ticker])\n",
        "    for i in range(len(ten_ks_by_ticker[ticker])):\n",
        "        fdate[i]=ten_ks_by_ticker[ticker][i]['file_date']\n",
        "    return fdate\n",
        "\n",
        "fdatek={}\n",
        "for ticker in ten_ks_by_ticker.keys():\n",
        "    fdatek[ticker]=get_file_dateK(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxxJTAYZJRm"
      },
      "outputs": [],
      "source": [
        "def get_FNproporK_date(fdatek,proporK):\n",
        "    FNproporKDATE={}\n",
        "    for ticker in fdatek.keys():\n",
        "        FNproporK_date= {fdatek[ticker][i]: proporK[ticker][i] for i in range(len(proporK[ticker]))}\n",
        "        FNproporKDATE.update(FNproporK_date)\n",
        "    return FNproporKDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FN_10K_portion=get_FNproporK_date(fdatek,FNproporK)    ###date: proportion of 10K negaword FinNeg\n",
        "#AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAaaaaaaaaaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colname=['Word'] \n",
        "H4N=pd.read_csv('Harvard IV_Negative Word List_Inf.txt',names=colname,header=None)\n",
        "\n",
        "H4N['negative']=True\n",
        "#H4N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiments = ['negative']\n",
        "\n",
        "#Fin_Neg=pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')\n",
        "#Fin_Neg=Fin_Neg.loc[Fin_Neg['Negative']>=2009]\n",
        "\n",
        "\n",
        "Hsentiment_df = H4N\n",
        "Hsentiment_df.columns = [column.lower() for column in Hsentiment_df.columns] # Lowercase the columns for ease of use\n",
        "\n",
        "# Remove unused information\n",
        "Hsentiment_df = Hsentiment_df[sentiments + ['word']]\n",
        "Hsentiment_df[sentiments] = Hsentiment_df[sentiments].astype(bool)\n",
        "Hsentiment_df = Hsentiment_df[(Hsentiment_df[sentiments]).any(1)]\n",
        "\n",
        "# Apply the same preprocessing to these words as the 10-k words\n",
        "Hsentiment_df['word'] = lemmatize_words(Hsentiment_df['word'].str.lower())\n",
        "Hsentiment_df = Hsentiment_df.drop_duplicates('word')\n",
        "\n",
        "\n",
        "Hsentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Hsentiment_bow_ten_ks = {}   #####setup ONCE?\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_k['file_clean_numb']) for ten_k in ten_ks]\n",
        "\n",
        "    Hsentiment_bow_ten_ks[ticker] = {sentiment: get_bag_of_words(Hsentiment_df[Hsentiment_df[sentiment]]['word'], lemma_docs)\n",
        "       for sentiment in sentiments}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Hget_negative_word_countK(ticker):\n",
        "    return np.cumsum(Hsentiment_bow_ten_ks[ticker]['negative'], axis=1)[:,-1]\n",
        "\n",
        "\n",
        "HproporK={}\n",
        "for ticker in ten_ks_by_ticker.keys():\n",
        "    HproporK[ticker]=Hget_negative_word_countK(ticker)/get_total_word_countK(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_HproporK_date(fdatek,HproporK):\n",
        "    HproporKDATE={}\n",
        "    for ticker in fdatek.keys():\n",
        "        HproporK_date= {fdatek[ticker][i]: HproporK[ticker][i] for i in range(len(HproporK[ticker]))}\n",
        "        HproporKDATE.update(HproporK_date)\n",
        "    return HproporKDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "H_10K_portion=get_HproporK_date(fdatek,HproporK) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_file_dateQ(ticker):\n",
        "    fdate=[0]*len(ten_Qs_by_ticker[ticker])\n",
        "    for i in range(len(ten_Qs_by_ticker[ticker])):\n",
        "        fdate[i]=ten_Qs_by_ticker[ticker][i]['file_date']\n",
        "    return fdate\n",
        "\n",
        "fdateQ={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    fdateQ[ticker]=get_file_dateQ(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_bow_ten_Qs = {}   #####setup ONCE?\n",
        "\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_Q['file_clean_numb']) for ten_Q in ten_Qs]\n",
        "\n",
        "    sentiment_bow_ten_Qs[ticker] = {sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
        "       for sentiment in sentiments}\n",
        "\n",
        "print_ten_k_data([sentiment_bow_ten_Qs[example_ticker]], sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_negative_word_countQ(ticker):\n",
        "    return np.cumsum(sentiment_bow_ten_Qs[ticker]['negative'], axis=1)[:,-1]\n",
        "\n",
        "def get_total_word_countQ(ticker):\n",
        "    total=[0]*len(ten_Qs_by_ticker[ticker])\n",
        "    for i in range(len(ten_Qs_by_ticker[ticker])):\n",
        "        total[i]=len(ten_Qs_by_ticker[ticker][i]['file_clean_numb'])\n",
        "    return total   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FNproporQ={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    FNproporQ[ticker]=get_negative_word_countQ(ticker)/get_total_word_countQ(ticker)\n",
        "#get_total_word_count('A')\n",
        "len(FNproporQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_file_dateQ(ticker):\n",
        "    fdate=[0]*len(ten_Qs_by_ticker[ticker])\n",
        "    for i in range(len(ten_Qs_by_ticker[ticker])):\n",
        "        fdate[i]=ten_Qs_by_ticker[ticker][i]['file_date']\n",
        "    return fdate\n",
        "\n",
        "fdateQ={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    fdateQ[ticker]=get_file_dateQ(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_FNproporQ_date(fdateQ,proporQ):\n",
        "    FNproporQDATE={}\n",
        "    for ticker in fdateQ.keys():\n",
        "        FNproporQ_date= {fdateQ[ticker][i]: proporQ[ticker][i] for i in range(len(proporQ[ticker]))}\n",
        "        FNproporQDATE.update(FNproporQ_date)\n",
        "    return FNproporQDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FN_10Q_portion=get_FNproporQ_date(fdateQ,FNproporQ)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Hsentiment_bow_ten_Qs = {}   #####setup ONCE?\n",
        "\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_Q['file_clean_numb']) for ten_Q in ten_Qs]\n",
        "\n",
        "    Hsentiment_bow_ten_Qs[ticker] = {sentiment: get_bag_of_words(Hsentiment_df[Hsentiment_df[sentiment]]['word'], lemma_docs)\n",
        "       for sentiment in sentiments}\n",
        "\n",
        "print_ten_k_data([Hsentiment_bow_ten_Qs[example_ticker]], sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Hget_negative_word_countQ(ticker):\n",
        "    return np.cumsum(Hsentiment_bow_ten_Qs[ticker]['negative'], axis=1)[:,-1]\n",
        "\n",
        "HproporQ={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    HproporQ[ticker]=Hget_negative_word_countQ(ticker)/get_total_word_countQ(ticker)\n",
        "    \n",
        "def get_HproporQ_date(fdateQ,HproporQ):\n",
        "    HproporQDATE={}\n",
        "    for ticker in fdateQ.keys():\n",
        "        HproporQ_date= {fdateQ[ticker][i]: HproporQ[ticker][i] for i in range(len(HproporQ[ticker]))}\n",
        "        HproporQDATE.update(HproporQ_date)\n",
        "    return HproporQDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "H_10Q_portion=get_HproporQ_date(fdateQ,HproporQ)\n",
        "H_10Q_portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###TFIDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def get_tfidf(sentiment_words, docs):\n",
        "    \n",
        "    vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
        "    tfidf = vec.fit_transform(docs)\n",
        "    \n",
        "    return tfidf.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##10K tfidf   FN\n",
        "\n",
        "FNsentiment_tfidf_ten_ks = {}\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
        "    \n",
        "        FNsentiment_tfidf_ten_ks[ticker] = {\n",
        "            sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
        "            for sentiment in sentiments}\n",
        "        \n",
        "    except:\n",
        "        print(ticker)\n",
        "print_ten_k_data([FNsentiment_tfidf_ten_ks[example_ticker]], sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FNsentiment_tfidf_ten_ks['OGN']={'negative':np.empty([5,1522])}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FN_tfidf_K={}\n",
        "for ticker in ten_ks_by_ticker.keys():\n",
        "    array=np.cumsum(FNsentiment_tfidf_ten_ks[ticker]['negative'], axis=1)[:,-1]\n",
        "    FN_tfidf_K[ticker]=array\n",
        "#FN_tfidf_K                                       ####### FN tfidf for 10K "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FN_tfidf_K_DATE={}\n",
        "for ticker in fdatek.keys():\n",
        "    try:\n",
        "        FNproporK_date= {fdatek[ticker][i]: FN_tfidf_K[ticker][i] for i in range(len(FN_tfidf_K[ticker]))}\n",
        "        FN_tfidf_K_DATE.update(FNproporK_date)\n",
        "    except:\n",
        "        # FNproporK_date=\n",
        "        print(ticker)\n",
        "FN_tfidf_K_DATE   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##10K tfidf   H4N\n",
        "\n",
        "Hsentiment_tfidf_ten_ks = {}\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    try:\n",
        "        lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
        "    \n",
        "        Hsentiment_tfidf_ten_ks[ticker] = {\n",
        "            sentiment: get_tfidf(Hsentiment_df[Hsentiment_df[sentiment]]['word'], lemma_docs)\n",
        "            for sentiment in sentiments}\n",
        "    except:\n",
        "        print(ticker)\n",
        "#print_ten_k_data([FNsentiment_tfidf_ten_ks[example_ticker]], sentiments)\n",
        "\n",
        "\n",
        "H_tfidf_K={}\n",
        "for ticker in ten_ks_by_ticker.keys():\n",
        "    try:\n",
        "        array=np.cumsum(Hsentiment_tfidf_ten_ks[ticker]['negative'], axis=1)[:,-1]\n",
        "        H_tfidf_K[ticker]=array\n",
        "    except:\n",
        "        print(ticker)\n",
        "#H_tfidf_K         \n",
        "\n",
        "\n",
        "H_tfidf_K_DATE={}\n",
        "for ticker in fdatek.keys():\n",
        "    try:\n",
        "        HproporK_date= {fdatek[ticker][i]: H_tfidf_K[ticker][i] for i in range(len(H_tfidf_K[ticker]))}\n",
        "        H_tfidf_K_DATE.update(HproporK_date)\n",
        "    except:\n",
        "        print(ticker)\n",
        "H_tfidf_K_DATE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_file_dateQ(ticker):\n",
        "    fdate=[0]*len(ten_Qs_by_ticker[ticker])\n",
        "    for i in range(len(ten_Qs_by_ticker[ticker])):\n",
        "        fdate[i]=ten_Qs_by_ticker[ticker][i]['file_date']\n",
        "    return fdate\n",
        "\n",
        "fdateQ={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    fdateQ[ticker]=get_file_dateQ(ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##10Q tfidf   FN\n",
        "\n",
        "FNsentiment_tfidf_ten_Qs = {}\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_Q['file_lemma']) for ten_Q in ten_Qs]\n",
        "    \n",
        "    FNsentiment_tfidf_ten_Qs[ticker] = {\n",
        "        sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
        "        for sentiment in sentiments}\n",
        "#print_ten_k_data([FNsentiment_tfidf_ten_Qs[example_ticker]], sentiments)\n",
        "\n",
        "\n",
        "FN_tfidf_Q={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    array=np.cumsum(FNsentiment_tfidf_ten_Qs[ticker]['negative'], axis=1)[:,-1]\n",
        "    FN_tfidf_Q[ticker]=array\n",
        "#FN_tfidf_Q                                       ####### FN tfidf for 10Q \n",
        "\n",
        "FN_tfidf_Q_DATE={}\n",
        "for ticker in fdateQ.keys():\n",
        "    FNproporQ_date= {fdateQ[ticker][i]: FN_tfidf_Q[ticker][i] for i in range(len(FN_tfidf_Q[ticker]))}\n",
        "    FN_tfidf_Q_DATE.update(FNproporQ_date)\n",
        "FN_tfidf_Q_DATE                                                      ####### FN tfidf for 10Q "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##10Q tfidf   H\n",
        "\n",
        "Hsentiment_tfidf_ten_Qs = {}\n",
        "for ticker, ten_Qs in ten_Qs_by_ticker.items():\n",
        "    lemma_docs = [' '.join(ten_Q['file_lemma']) for ten_Q in ten_Qs]\n",
        "    \n",
        "    Hsentiment_tfidf_ten_Qs[ticker] = {\n",
        "        sentiment: get_tfidf(Hsentiment_df[Hsentiment_df[sentiment]]['word'], lemma_docs)\n",
        "        for sentiment in sentiments}\n",
        "#print_ten_k_data([Hsentiment_tfidf_ten_Qs[example_ticker]], sentiments)\n",
        "\n",
        "\n",
        "H_tfidf_Q={}\n",
        "for ticker in ten_Qs_by_ticker.keys():\n",
        "    array=np.cumsum(Hsentiment_tfidf_ten_Qs[ticker]['negative'], axis=1)[:,-1]\n",
        "    H_tfidf_Q[ticker]=array\n",
        "#FN_tfidf_Q                                       ####### H tfidf for 10Q \n",
        "\n",
        "H_tfidf_Q_DATE={}\n",
        "for ticker in fdateQ.keys():\n",
        "    HproporQ_date= {fdateQ[ticker][i]: H_tfidf_Q[ticker][i] for i in range(len(H_tfidf_Q[ticker]))}\n",
        "    H_tfidf_Q_DATE.update(HproporQ_date)\n",
        "H_tfidf_Q_DATE                                                      ####### H tfidf for 10Q "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c10k= [H_10K_portion, FN_10K_portion,H_tfidf_K_DATE,FN_tfidf_K_DATE]\n",
        "K={}\n",
        "\n",
        "for k in c10k[0].keys():\n",
        "  K[k] = list(d[k] for d in c10k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "c10Q= [H_10Q_portion,FN_10Q_portion,H_tfidf_Q_DATE,FN_tfidf_Q_DATE]\n",
        "Q={}\n",
        "\n",
        "for k in c10Q[0].keys():\n",
        "  Q[k] = list(d[k] for d in c10Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfK = pd.DataFrame.from_dict(K).T\n",
        "dfQ=pd.DataFrame.from_dict(Q).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frames = [dfK,dfQ]\n",
        "\n",
        "DF0_50= pd.concat(frames)\n",
        "DF0_50.columns = ['H4N_portion','FN_portion','H_tfidf','FN_tfidf']\n",
        "DF0_50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DF0_50.to_csv('DF_451_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "stock = pd.read_csv(\"stock master.csv\",dtype = {'tic': str})\n",
        "stock['datadate'] = pd.to_datetime(stock['datadate'], format='%Y%m%d')\n",
        "\n",
        "spy_return = pd.read_csv(\"sp500 daily returns tristan.csv\")\n",
        "spy_return['caldt'] = pd.to_datetime(spy_return['caldt'], format='%Y%m%d')\n",
        "vwr = spy_return['vwretd'] + 1\n",
        "vwr1 = vwr.shift(1)\n",
        "vwr2 = vwr.shift(2)\n",
        "\n",
        "sst = -stock.groupby(['tic'])['prccd'].diff(-4)/stock['prccd']\n",
        "datadic = {'date':stock['datadate'],'tic':stock['tic'], 'sst':sst}\n",
        "stock_new = pd.DataFrame(datadic)\n",
        "\n",
        "spy_return_ret = vwr*vwr1*vwr2-1\n",
        "spy_return_ret = spy_return_ret.shift(-3)\n",
        "datadic = {'date': spy_return['caldt'], 'idx': spy_return_ret}\n",
        "spy_return_new = pd.DataFrame(datadic)\n",
        "\n",
        "excess_return = stock_new.merge(spy_return_new, on = 'date')\n",
        "excess_return = excess_return.dropna()\n",
        "excess_return = excess_return.sort_values(by=['tic', 'date'])\n",
        "excess_return['excess_return'] = excess_return['sst'] - excess_return['idx']\n",
        "\n",
        "def get_return(ticker, date_str):\n",
        "    year = date_str[:4] # '2021-12-17'\n",
        "    month = date_str[5:7]\n",
        "    day = date_str[8:10]\n",
        "    # print(year, month, day)\n",
        "    date = datetime(int(year), int(month), int(day))\n",
        "    \n",
        "    ret = excess_return[excess_return['tic']==ticker]\n",
        "    ret = ret.drop_duplicates(subset=['date'])\n",
        "    ret = ret.set_index('date')\n",
        "    pos = ret.index.get_loc(date, method='bfill')\n",
        "    ret = ret.iloc[pos]['excess_return']\n",
        "    \n",
        "    return ret\n",
        "\n",
        "\n",
        "# print(excess_return['tic'])\n",
        "#print(get_return('A', '2021-12-17'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "excess_returnK={}\n",
        "for ticker,dates in fdatek.items():\n",
        "    for date in dates:\n",
        "        try:\n",
        "            excess_returnK[date]=get_return(ticker, str(date))\n",
        "        except:\n",
        "            excess_returnK[date]=0.0\n",
        "#excess_returnK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "excess_returnQ={}\n",
        "for ticker,dates in fdateQ.items():\n",
        "    for date in dates:\n",
        "        try:\n",
        "            excess_returnQ[date]=get_return(ticker, str(date))\n",
        "        except:\n",
        "            excess_returnQ[date]=0.0\n",
        "excess_returnQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(excess_returnK,orient='index')\n",
        "df2= pd.DataFrame.from_dict(excess_returnQ,orient='index')\n",
        "df3 = df.append(df2)\n",
        "DF0_50_c = DF0_50\n",
        "DF0_50_c['return'] = df3\n",
        "DF0_50_c.to_csv('DF0_' + str(num) + '.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
